{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+scru3Tk8zWXZUNjpE5sf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ezAKSv7O6PRw"},"source":["# 08. Natural Language Processing with TensorFlow\n","\n","The main goal of [natural language processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32) (NLP) is to derive information from natural language.\n","\n","Natural language is a broad term but you can consider it to cover any of the following:\n","\n","* Text (such as that contained in an email, blog post, book, Tweet)\n","* Speech (a conversation you have with a doctor, voice commands you give to a smart speaker)\n","\n","Under the umbrellas of text and speech there are many different things you might want to do.\n","\n","If you're building an email application, you might want to scan incoming emails to see if they're spam or not spam (classification).\n","\n","If you're trying to analyse customer feedback complaints, you might want to discover which section of your business they're for\n","\n","> 🔑 Note: Both of these types of data are often referred to as sequences (a sentence is a sequence of words). So a common term you'll come across in NLP problems is called seq2seq, in other words, finding information in one sequence to produce another sequence (e.g. converting a speech command to a sequence of text-based steps).\n","\n","To get hands-on with NLP in TensorFlow, we're going to practice the steps we've used previously but this time with text data:\n","\n","  - Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)\n","\n","> 📖 Resource: For a great overview of NLP and the different problems within it, read the article A [Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)."]},{"cell_type":"markdown","metadata":{"id":"a-HUpvBy8UO0"},"source":["# What we're going to cover\n","\n","Let's get specific hey?\n","\n","* Downloading a text dataset\n","* Visualizing text data\n","* Converting text into numbers using tokenization\n","* Turning our tokenized text into an embedding\n","* Modelling a text dataset\n","  * Starting with a baseline (TF-IDF)\n","  * Building several deep learning text models\n","    * Dense, LSTM, GRU, Conv1D, Transfer Learning\n","* Comparing the performance of each our models\n","* Combining our models into an ensemble\n","* Saving and loading a trained model\n","* Find the most wrong predictions"]},{"cell_type":"markdown","metadata":{"id":"tA_yy3cT9Xbb"},"source":["# Check for GPU"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_0nvAvO59iyO","executionInfo":{"status":"ok","timestamp":1631868869610,"user_tz":-420,"elapsed":384,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"2c52dadd-bc02-4a75-d44b-c0fa91052292"},"source":["# Check for GPU\n","!nvidia-smi -L"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla K80 (UUID: GPU-34464f65-54fd-60fc-09ce-f5c52cb2403c)\n"]}]},{"cell_type":"markdown","metadata":{"id":"tJ-gaTrQ9l4h"},"source":["# Get helper functions"]},{"cell_type":"code","metadata":{"id":"a3RFzdzk9uIp"},"source":["\n","# Download helper functions script\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOScxE799xOv"},"source":["# Import series of helper functions for the notebook\n","from helper_functions import unzip_data, create_tensorboard_callback, plot_loss_curves, compare_historys"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBcCWpIH96ga"},"source":["# Download a text dataset\n","\n","Let's start by download a text dataset. We'll be using the [Real or Not?](https://www.kaggle.com/c/nlp-getting-started/data) dataset from Kaggle which contains text-based Tweets about natural disasters.\n","\n","The Real Tweets are actually about diasters, for example:\n","\n","  Jetstar and Virgin forced to cancel Bali flights again because of ash from Mount Raung volcano\n","\n","The Not Real Tweets are Tweets not about diasters (they can be on anything), for example:\n","\n","  'Education is the most powerful weapon which you can use to change the world.' Nelson #Mandela #quote\n","\n","For convenience, the dataset has been [downloaded](https://www.kaggle.com/c/nlp-getting-started/data) from Kaggle (doing this requires a Kaggle account) and uploaded as a downloadable zip file.\n","\n","> 🔑 Note: The original downloaded data has not been altered to how you would download it from Kaggle."]},{"cell_type":"code","metadata":{"id":"zHosWha399Xa"},"source":["# Download the data\n","!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n","\n","# Unzip data\n","unzip_data(\"nlp_getting_started.zip\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nhUtCKOF--kZ"},"source":["Unzipping nlp_getting_started.zip gives the following 3 .csv files:\n","\n","* sample_submission.csv - an example of the file you'd submit to the Kaggle competition of your model's predictions.\n","* train.csv - training samples of real and not real diaster Tweets.\n","* test.csv - testing samples of real and not real diaster Tweets."]},{"cell_type":"markdown","metadata":{"id":"pjXYY1xE-38w"},"source":["# Visualizing a text dataset"]},{"cell_type":"markdown","metadata":{"id":"U6RrXo9e-9ci"},"source":["Once you've acquired a new dataset to work with, what should you do first?\n","\n","Explore it? Inspect it? Verify it? Become one with it?\n","\n","All correct.\n","\n","Remember the motto: visualize, visualize, visualize.\n","\n","Right now, our text data samples are in the form of .csv files. For an easy way to make them visual, let's turn them into pandas DataFrame's.\n","\n","> 📖 Reading: You might come across text datasets in many different formats. Aside from CSV files (what we're working with), you'll probably encounter .txt files and .json files too. For working with these type of files, I'd recommend reading the two following articles by RealPython:\n","  - [How to read files in python](https://realpython.com/read-write-files-python/)\n","  - [Working with JSON Data in Python](https://realpython.com/python-json/)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"4Nno2aFo_d8m","executionInfo":{"status":"ok","timestamp":1631869494619,"user_tz":-420,"elapsed":270,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"c96bc1a1-14b8-4506-fd36-f6e399701a2b"},"source":["# Turn .csv files into pandas Dataframe\n","import pandas as pd\n","train_df = pd.read_csv(\"train.csv\")\n","test_df = pd.read_csv(\"test.csv\")\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ...                                               text target\n","0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n","1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n","2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n","3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n","4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"B8ycf18QAGGK"},"source":["The training data we downloaded is probably shuffled already. But just to be sure, let's shuffle it again.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"8QS_6aTYAQd1","executionInfo":{"status":"ok","timestamp":1631869606492,"user_tz":-420,"elapsed":400,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"60bb5e97-7d17-4cbd-f8e7-46e131203f3c"},"source":["# Shuffle trainnig dataframe\n","train_df_shuffled = train_df.sample(frac=1, random_state=42)\n","train_df_shuffled.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2644</th>\n","      <td>3796</td>\n","      <td>destruction</td>\n","      <td>NaN</td>\n","      <td>So you have a new weapon that can cause un-ima...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2227</th>\n","      <td>3185</td>\n","      <td>deluge</td>\n","      <td>NaN</td>\n","      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5448</th>\n","      <td>7769</td>\n","      <td>police</td>\n","      <td>UK</td>\n","      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>191</td>\n","      <td>aftershock</td>\n","      <td>NaN</td>\n","      <td>Aftershock back to school kick off was great. ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6845</th>\n","      <td>9810</td>\n","      <td>trauma</td>\n","      <td>Montgomery County, MD</td>\n","      <td>in response to trauma Children of Addicts deve...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        id  ... target\n","2644  3796  ...      1\n","2227  3185  ...      0\n","5448  7769  ...      1\n","132    191  ...      0\n","6845  9810  ...      0\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"I5p_K2EwAhVb"},"source":["Notice how the training data has a \"target\" column.\n","\n","We're going to be writing code to find patterns (e.g. different combinations of words) in the \"text\" column of the training dataset to predict the value of the \"target\" column.\n","\n","The test dataset doesn't have a \"target\" column.\n","\n","    Inputs (text column) -> Machine Learning Algorithm -> Outputs (target column)\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"ZSQDAdl-A3Nl","executionInfo":{"status":"ok","timestamp":1631869781247,"user_tz":-420,"elapsed":292,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"83a2e40f-c36a-401d-924f-d86b3923daeb"},"source":["# The test data doesn't have a target ( that's what we'd try to predict )\n","test_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text\n","0   0     NaN      NaN                 Just happened a terrible car crash\n","1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n","2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n","3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n","4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"cN7xmHj0BK9R"},"source":["Let's check how many examples of each target we have"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yvheC0ZSBQ2Q","executionInfo":{"status":"ok","timestamp":1631869830302,"user_tz":-420,"elapsed":285,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"4c9fd06a-2cc7-4a3c-da76-030811b89601"},"source":["# How many examples of each class\n","train_df['target'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    4342\n","1    3271\n","Name: target, dtype: int64"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"TkSGr2DIBYHE"},"source":["Since we have two target values, we're dealing with a binary classification problem.\n","\n","It's fairly balanced too, about 60% negative class (target = 0) and 40% positive class (target = 1).\n","\n","Where,\n","\n","* 1 = a real disaster Tweet\n","* 0 = not a real disaster Tweet\n","\n","And what about the total number of samples we have?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rh_RE3XhBhFP","executionInfo":{"status":"ok","timestamp":1631869944667,"user_tz":-420,"elapsed":364,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"2729ce88-22aa-4ee9-cf3d-5a36e7600000"},"source":["# How many sample total\n","print(f\"Total training samples: {len(train_df)}\")\n","print(f\"Total testing samples: {len(test_df)}\")\n","print(f\"Total samples: {len(train_df) + len(test_df) }\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total training samples: 7613\n","Total testing samples: 3263\n","Total samples: 10876\n"]}]},{"cell_type":"markdown","metadata":{"id":"t9OPrEmyBz4l"},"source":["Alright, seems like we've got a decent amount of training and test data. If anything, we've got an abundance of testing examples, usually a split of 90/10 (90% training, 10% testing) or 80/20 is suffice.\n","\n","Okay, time to visualize, let's write some code to visualize random text samples.\n","\n","> 🤔 Question: Why visualize random samples? You could visualize samples in order but this could lead to only seeing a certain subset of data. Better to visualize a substantial quantity (100+) of random samples to get an idea of the different kinds of data you're working with. In machine learning, never underestimate the power of randomness."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXb7jG7-B8Sb","executionInfo":{"status":"ok","timestamp":1631870397679,"user_tz":-420,"elapsed":282,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"5bb0eb55-07b2-4748-bc72-2f50dd481d3a"},"source":["# Let's visualize some random training examples\n","import random\n","\n","random_index = random.randint(0, len(train_df) - 5) # Create random indexes not higher than the total number of samples\n","\n","for row in train_df_shuffled[[\"text\",\"target\"]][random_index:random_index+5].itertuples(): # itertuples : Iterate all DataFrame: lap di lap lai\n","  _, text, target = row\n","  print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n","  print(f\"Text:\\n{text}\\n\")\n","  print(\"---\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Target: 0 (not real disaster)\n","Text:\n","When you're in deep sleep and then you dream you're bout to fall off a cliff then wake up while struggling to keep a balance\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","The Next Financial Crash. 'The Writing is on the Wall'. Don't Say 'You Weren't Warned' http://t.co/H7lDx29aba\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","Damn there's really no MLK center that hasn't sunk in yet\n","\n","---\n","\n","Target: 1 (real disaster)\n","Text:\n","Refugio oil spill may have been costlier bigger than projected http://t.co/41L8tqCAey\n","\n","---\n","\n","Target: 0 (not real disaster)\n","Text:\n","Brian Shaw + J.J. Hickson + Kenneth Faried trying to defend LaMarcus Aldridge was A BLOOD VOLCANO http://t.co/20TWGPmM7d\n","\n","---\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9_bZ6FOcCNHN"},"source":["# Split data into training and validation sets\n","\n","Since the test set has no labels and we need a way to evalaute our trained models, we'll split off some of the training data and create a validation set.\n","\n","When our model trains (tries patterns in the Tweet samples), it'll only see data from the training set and we can see how it performs on unseen data using the validation set.\n","\n","We'll convert our splits from pandas Series datatypes to lists of strings (for the text) and lists of ints (for the labels) for ease of use later.\n","\n","To split our training dataset and create a validation dataset, we'll use Scikit-Learn's `train_test_split()` method and dedicate 10% of the training samples to the validation set.\n"]},{"cell_type":"code","metadata":{"id":"NmEBB5HID4Sp"},"source":["from sklearn.model_selection import train_test_split\n","\n","# Use train_test_split to split training data into training and validation sets\n","\n","train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n","                                                                            train_df_shuffled[\"target\"].to_numpy(),\n","                                                                            test_size=0.1,\n","                                                                            random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCe7LRMRE2pV","executionInfo":{"status":"ok","timestamp":1631870877757,"user_tz":-420,"elapsed":263,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"921d76cb-e0e2-453f-a600-f08ffb2e526c"},"source":["# Check the lengths\n","len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6851, 6851, 762, 762)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YLHPsyxFOC1","executionInfo":{"status":"ok","timestamp":1631870947728,"user_tz":-420,"elapsed":267,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"c01986df-8572-403f-8f59-861a8c7388d0"},"source":["# View the first 10 training sentences and thiers label\n","train_sentences[:10], train_labels[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n","        'Imagine getting flattened by Kurt Zouma',\n","        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n","        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n","        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n","        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n","        'destroy the free fandom honestly',\n","        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n","        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n","        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n","       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"6f59X1NXFo0N"},"source":["# Converting text into numbers \n","\n","Wonderful! We've got a training set and a validation set containing Tweets and labels.\n","\n","Our labels are in numerical form (0 and 1) but our Tweets are in string form.\n","\n","> 🤔 Question: What do you think we have to do before we can use a machine learning algorithm with our text data?\n","\n","If you answered something along the lines of \"turn it into numbers\", you're correct. A machine learning algorithm requires its inputs to be in numerical form.\n","\n","In NLP, there are two main concepts for turning text into numbers:\n","* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n","  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being 0, \"love\" being 1 and \"TensorFlow\" being 2. In this case, every word in a sequence considered a single token.\n","  2. **Character-level tokenization**, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single **token**.\n","  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n","\n","* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n","  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as tf.keras.layers.Embedding) and an embedding representation will be learned during model training.\n","  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task.\n","  \n","> 🤔 Question: What level of tokenzation should I use? What embedding should should I choose?\n","\n","It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best. You might even want to try stacking them (e.g. combining the outputs of your embedding layers using `tf.keras.layers.concatenate`).\n","\n","If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start."]},{"cell_type":"markdown","metadata":{"id":"ibA4VgCHGGtw"},"source":["# Text vectorization (Tokenization)\n","\n","Enough talking about tokenization and embeddings, let's create some.\n","\n","We'll practice tokenzation (mapping our words to numbers) first.\n","\n","To tokenize our words, we'll use the helpful preprocessing layer `tf.keras.layers.experimental.preprocessing.TextVectorization`.\n","\n","The TextVectorization layer takes the following parameters:\n","\n","* max_tokens - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens\n","* standardize - Method for standardizing text. Default is \"lower_and_strip_punctuation\" which lowers text and removes all punctuation marks.\n","* split - How to split text, default is \"whitespace\" which splits on spaces.\n","* output_mode - How to output tokens, can be \"int\" (integer mapping), \"binary\" (one-hot encoding), \"count\" or \"tf-idf\". See documentation for more.\n","* output_sequence_length - Length of tokenized sequence to output. For example, if output_sequence_length=150, all tokenized sequences will be 150 tokens long.\n","* pad_to_max_tokens - Defaults to False, if True, the output feature axis will be padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens. Only valid in certain modes, see docs for more.\n","\n","Let's see it in action."]},{"cell_type":"code","metadata":{"id":"RGEggmEWIQvv"},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization\n","\n","# Note: in TensorFlow 2.6+, you no longer need \"layers.experimental.preprocessing\"\n","# you can use: \"tf.keras.layers.TextVectorization\", see https://github.com/tensorflow/tensorflow/releases/tag/v2.6.0 for more\n","\n","# Use the default Textvectorize variables:\n","text_vectorizer = TextVectorization(max_tokens=None, # How many words in the voca (all of the different words in ur text)\n","                                    standardize=\"lower_and_strip_punctuation\", # How to process text\n","                                    split=\"whitespace\", # How to split tokens\n","                                    ngrams=None, # Create groups of n-words\n","                                    output_mode=\"int\", # How to map token to numbers\n","                                    output_sequence_length=None) # How long should the output sequence of tokens be?\n","                                    #pad_to_max_tokens=True # Not valid if using max_tokens=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hIzNoSw9KyNj"},"source":["We've initialized a TextVectorization object with the default settings but let's customize it a little bit for our own use case.\n","\n","In particular, let's set values for max_tokens and output_sequence_length.\n","\n","For max_tokens (the number of words in the vocabulary), multiples of 10,000 (10,000, 20,000, 30,000) or the exact number of unique words in your text (e.g. 32,179) are common values.\n","\n","For our use case, we'll use 10,000.\n","\n","And for the output_sequence_length we'll use the average number of tokens per Tweet in the training set. But first, we'll need to find it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Staj3F_zK-Uw","executionInfo":{"status":"ok","timestamp":1631872445772,"user_tz":-420,"elapsed":266,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"c4e9ff20-9f36-47f4-d8f0-9a5e5ad6610d"},"source":["# Find average number of tokens (words) in Training tweetes\n","round(sum([len(i.split()) for i in train_sentences])/ len(train_sentences))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"nGRILrpJLWOy"},"source":["Now let's create another TextVectorization object using our custom parameters.\n","\n"]},{"cell_type":"code","metadata":{"id":"9Sg7r4e0LyuG"},"source":["# Setup text vectorization with custom variables\n","max_vocab_length = 10000 # Max number of words to have in our vocabulary\n","max_length = 15 # Max length our sequnces will be (e.g. how many words from a tweet does our model see?)\n","\n","text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n","                                    output_mode=\"int\",\n","                                    output_sequence_length=max_length)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CxB7xxzMQun"},"source":["\n","Beautiful!\n","\n","To map our TextVectorization instance text_vectorizer to our data, we can call the `adapt()` method on it whilst passing it our training text."]},{"cell_type":"code","metadata":{"id":"wFXXU4QnMVXf"},"source":["# Fit the text vectorizer to traning text\n","text_vectorizer.adapt(train_sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aftplopiMZ1l"},"source":["Training data mapped! Let's try our text_vectorizer on a custom sentence (one similar to what you might see in the training data).\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEE32sSDMduX","executionInfo":{"status":"ok","timestamp":1631872769354,"user_tz":-420,"elapsed":280,"user":{"displayName":"Minh Nghĩa","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjMxpzwEK5qLe8Rk72kieD5N2lNcwCEi1EWIUeqVw=s64","userId":"07810955240880502665"}},"outputId":"cc7f3dfb-33c7-4abb-c551-b2c228ac0bc6"},"source":["# Create a sample sentence and tokenize it\n","sample_sentence = \"There's a flood in my street!\"\n","text_vectorizer([sample_sentence])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n","array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0]])>"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"IB-uZdDFMln-"},"source":["Wonderful, it seems we've got a way to turn our text into numbers (in this case, word-level tokenization). Notice the 0's at the end of the returned tensor, this is because we set output_sequence_length=15, meaning no matter the size of the sequence we pass to text_vectorizer, it always returns a sequence with a length of 15.\n","\n","How about we try our text_vectorizer on a few random sentences?"]},{"cell_type":"code","metadata":{"id":"r_yqd9xrMwi4"},"source":[],"execution_count":null,"outputs":[]}]}